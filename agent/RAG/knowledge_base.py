#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
retriever.py  â€”  ç›®å½•ä¸‹ TXT / PDF å‘é‡åº“åˆå§‹åŒ–è„šæœ¬

ä»…è´Ÿè´£ï¼š
  1. è¯»å–ç›®å½•ä¸‹æ‰€æœ‰ .txt/.pdf æ–‡ä»¶
  2. åˆ‡å—å¹¶ç”ŸæˆåµŒå…¥
  3. æŒä¹…åŒ–åˆ° Chroma å‘é‡åº“
  4. æ›´æ–°èµ„æ–™ï¼Œè¯·åˆ é™¤chroma.sqlite3

ä¸åŒ…å« LLM è°ƒç”¨æˆ–é—®ç­”åŠŸèƒ½ã€‚
ç”± RAG ä¸Šå±‚é€»è¾‘è°ƒç”¨ã€‚
"""
import os
from pathlib import Path
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader, PyPDFLoader, UnstructuredPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(override=True)

# ---------------------------------------------------------------------------
# åŸºæœ¬é…ç½® - ä»ç¯å¢ƒå˜é‡è¯»å–
DOC_DIR = Path(os.getenv("DOC_DIR"))          # æ–‡æ¡£ç›®å½•
VECTOR_DIR = Path(os.getenv("VECTOR_DIR"))    # å‘é‡åº“å­˜æ”¾ç›®å½•
COLLECTION = os.getenv("COLLECTION_NAME")     # å‘é‡é›†åˆåç§°
EMBED_MODEL = os.getenv("EMBED_MODEL")        # åµŒå…¥æ¨¡å‹

api_key = os.getenv("OPENAI_API_KEY")
api_base = os.getenv("OPENAI_API_BASE")

# åµŒå…¥ä¸åˆ‡å—é…ç½®
embeddings = OpenAIEmbeddings(
    model=EMBED_MODEL,
    openai_api_key=api_key,
    openai_api_base=api_base,
)
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1200,
    chunk_overlap=150,
    separators=["\n\n", "\n", "ã€‚", ".", "!", "?", " "],
)

def load_docs_from_path(path: Path):
    """
    å•æ–‡ä»¶åŠ è½½å¹¶åˆ‡åˆ†ï¼Œæ”¯æŒ .txt å’Œ .pdfï¼ˆå«æ‰«æç‰ˆï¼‰ã€‚
    è¿”å› List[Document]
    """
    ext = path.suffix.lower()
    if ext == ".txt":
        loader = TextLoader(str(path), encoding="utf-8")
    elif ext == ".pdf":
        try:
            loader = PyPDFLoader(str(path))
        except Exception:
            loader = UnstructuredPDFLoader(str(path), mode="elements")
    else:
        return []
    return splitter.split_documents(loader.load())

def load_all_docs(dir_path: Path):
    """
    éå†ç›®å½•ä¸‹æ‰€æœ‰ .txt/.pdf æ–‡ä»¶å¹¶åˆ‡åˆ†ï¼Œè¿”å›æ‰€æœ‰æ–‡æ¡£å—ã€‚
    """
    docs = []
    for file in dir_path.iterdir():
        if file.is_file() and file.suffix.lower() in {".txt", ".pdf"}:
            docs.extend(load_docs_from_path(file))
    return docs


def init_vectorstore():
    """
    åˆå§‹åŒ–å‘é‡åº“ï¼š
    - å¦‚æœå‘é‡åº“ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„å‘é‡åº“
    - å¦‚æœå‘é‡åº“å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›ç°æœ‰å®ä¾‹
     
    å¦‚æœéœ€è¦é‡å»ºï¼Œè¯·å…ˆåˆ é™¤ VECTOR_DIR ç›®å½•
    """
    # å¦‚æœå‘é‡åº“å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
    if VECTOR_DIR.exists() and any(VECTOR_DIR.iterdir()):
        print(f"âœ” å‘é‡åº“å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½")
        vs = Chroma(
            embedding_function=embeddings,
            persist_directory=str(VECTOR_DIR),
            collection_name=COLLECTION,
        )
        print(f"âœ” å‘é‡åº“åŠ è½½å®Œæˆï¼Œå…± {vs._collection.count()} chunks")
        return vs
     
    # åˆ›å»ºæ–°çš„å‘é‡åº“
    print("ğŸ”„ å¼€å§‹æ„å»ºå‘é‡åº“...")
    docs = load_all_docs(DOC_DIR)
     
    if not docs:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
        return None
    
    print(f"ğŸ“š æ€»å…±éœ€è¦å¤„ç† {len(docs)} ä¸ªæ–‡æ¡£å—")
    
    # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å•æ¬¡è¯·æ±‚tokenè¿‡å¤š
    batch_size = 100  # æ¯æ‰¹100ä¸ªæ–‡æ¡£å—ï¼Œå¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
    vs = None
    
    try:
        for i in range(0, len(docs), batch_size):
            batch_docs = docs[i:i+batch_size]
            current_batch = i // batch_size + 1
            total_batches = (len(docs) + batch_size - 1) // batch_size
            
            print(f"ğŸ“¦ å¤„ç†ç¬¬ {current_batch}/{total_batches} æ‰¹ï¼Œå…± {len(batch_docs)} ä¸ªæ–‡æ¡£å—")
            
            if vs is None:
                # ç¬¬ä¸€æ‰¹ï¼šåˆ›å»ºå‘é‡åº“
                vs = Chroma.from_documents(
                    batch_docs,
                    embeddings,
                    persist_directory=str(VECTOR_DIR),
                    collection_name=COLLECTION,
                )
                print(f"âœ… ç¬¬1æ‰¹å¤„ç†å®Œæˆï¼Œå·²åˆ›å»ºå‘é‡åº“")
            else:
                # åç»­æ‰¹æ¬¡ï¼šæ·»åŠ åˆ°ç°æœ‰å‘é‡åº“
                vs.add_documents(batch_docs)
                print(f"âœ… ç¬¬{current_batch}æ‰¹å¤„ç†å®Œæˆ")
            
            # æ¯æ‰¹å¤„ç†åéƒ½ä¿å­˜
            vs.persist()
        
        final_count = vs._collection.count()
        print(f"ğŸ‰ å‘é‡åº“æ„å»ºå®Œæˆï¼")
        print(f"ğŸ“Š æ€»æ–‡æ¡£å—æ•°: {final_count}")
        print(f"ğŸ“Š å¤„ç†æ‰¹æ¬¡æ•°: {total_batches}")
        return vs
        
    except Exception as e:
        print(f"âŒ å‘é‡åº“æ„å»ºå¤±è´¥: {e}")
        # å¦‚æœæ„å»ºå¤±è´¥ï¼Œæ¸…ç†å¯èƒ½çš„æ®‹ç•™æ–‡ä»¶
        if VECTOR_DIR.exists():
            import shutil
            try:
                shutil.rmtree(VECTOR_DIR)
                print("ğŸ§¹ å·²æ¸…ç†å¤±è´¥çš„å‘é‡åº“æ–‡ä»¶")
            except:
                pass
        return None
    """
    åˆå§‹åŒ–å‘é‡åº“ï¼š
    - å¦‚æœå‘é‡åº“ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„å‘é‡åº“
    - å¦‚æœå‘é‡åº“å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›ç°æœ‰å®ä¾‹
    
    å¦‚æœéœ€è¦é‡å»ºï¼Œè¯·å…ˆåˆ é™¤ VECTOR_DIR ç›®å½•
    """
    # å¦‚æœå‘é‡åº“å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
    if VECTOR_DIR.exists() and any(VECTOR_DIR.iterdir()):
        print(f" å‘é‡åº“å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½")
        vs = Chroma(
            embedding_function=embeddings,
            persist_directory=str(VECTOR_DIR),
            collection_name=COLLECTION,
        )
        print(f"âœ” å‘é‡åº“åŠ è½½å®Œæˆï¼Œå…± {vs._collection.count()} chunks")
        return vs
    
    # åˆ›å»ºæ–°çš„å‘é‡åº“
    print("å¼€å§‹æ„å»ºå‘é‡åº“...")
    docs = load_all_docs(DOC_DIR)
    
    if not docs:
        print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
        return None
    
    vs = Chroma.from_documents(
        docs,
        embeddings,
        persist_directory=str(VECTOR_DIR),
        collection_name=COLLECTION,
    )
    vs.persist()
    print(f"âœ” å‘é‡åº“æ„å»ºå®Œæˆï¼Œå…± {len(docs)} chunks")
    return vs